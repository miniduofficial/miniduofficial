[
  {
    "id": "Contact.html",
    "title": "The Study",
    "url": "./Contact.html",
    "tags": [],
    "date": "",
    "excerpt": "Within the pages of my blog you will find an ever changing archive of my curiosities--a space borne of thought where I publish thoughts and creations stemming from subjects that captivate me…",
    "content": "Greetings, weary surfer of the web. Within the pages of my blog you will find an ever changing archive of my curiosities--a space borne of thought where I publish thoughts and creations stemming from subjects that captivate me. This space is devoid of the shackles of genre but is only a procession of essays, passages, reports, reflections, and poems--each a thread in the tapestry of my never ending inquiry. A sort of Camusian process. As I am subjected to change in this realm of impermanence, so is this blog ever in flux, never being fully complete. You are warmly invited to ponder the matters under discourse here. I bid you an insightful day."
  },
  {
    "id": "Determinism.html",
    "title": "Free Will",
    "url": "./Determinism.html",
    "tags": [],
    "date": "",
    "excerpt": "          Free Will Home Essays     The Illusion of Free Will",
    "content": "Loading... Epicentre Articulations Fragments The Study Free Will Home Essays Fragments The Study The Illusion of Free Will"
  },
  {
    "id": "Essays.html",
    "title": "Articulations",
    "url": "./Essays.html",
    "tags": [],
    "date": "",
    "excerpt": "Here, I present works shaped by deliberate articulation and contemplative design—each brought to a semblance of completion. As is the case with all content on this blog, it too lacks a strict thematic design, instead drawing from disparate domains…",
    "content": "Foreword Here, I present works shaped by deliberate articulation and contemplative design—each brought to a semblance of completion. As is the case with all content on this blog, it too lacks a strict thematic design, instead drawing from disparate domains. This modest corner within my blog—my brainchild—is thus reserved for such works that may quench your thirst for arcane yet meaningful inquiry, though pieces of a more colloquial nature will also find refuge here. Rendered Conceptions The fall of God Cavorting with Univariate Linear Regression"
  },
  {
    "id": "General Intelligence Definition.html",
    "title": "General Intelligence",
    "url": "./General Intelligence Definition.html",
    "tags": [],
    "date": "",
    "excerpt": "The psychometric view of “general intelligence” being a latent scalar that evaluates an agent’s ability to complete a broad range of tasks has begun to feel less and less plausible to me which is a sentiment shared among those within the field of AI as well…",
    "content": "General Intelligence as a Manifold Over a Task Space The psychometric view of “general intelligence” being a latent scalar that evaluates an agent’s ability to complete a broad range of tasks has begun to feel less and less plausible to me which is a sentiment shared among those within the field of AI as well. The idea that a single score is reflective of an agent’s ability across multiple domains hints to me as being a bit reductive. Perhaps general intelligence is not a quantity but a structure? Human capability, in the real world, is not simply limited to a select few domains. An expert who has specialised in one domain could at the same time perform competently at a task that is similar to their field of expertise and even tasks that are well outside of the neighbourhood of their domain, for example, a mathematician cooking or a musician doing logical reasoning. Their skill varies when moving from task to task and provided that we move in a way that each successive task is similar to the preceding one, my intuition compels me to believe that this variation is continuous rather than there being sudden spikes and steep valleys. If we imagine a landscape with tasks laid out in such a way that those similar are placed in each other’s neighbourhood, perhaps a person’s competence across the terrain would resemble a continuous surface instead of disconnected spikes. Lately, I find myself imagining general intelligence, not as a score but rather as a manifold-like structure over the task terrain. It still remains an intuition, unsubstantiated and unproven yet it also feels compelling, at least to me. If anything like this picture turned out to be true, then it would raise the question of what the geometry of the manifold would reveal about the way in which competence flows between domains. Something which felt resonant with this intuition was the paper on “fractured entangled representation” by Stanley et al. [1] Perhaps not directly associated with this manifold hypothesis, it explores how structure isn’t represented coherently within contemporary large models. A tiny perturbation to a weight results in a chaotic and incoherent change to what the models trained using SGD were generating. By contrast, the serendipitous process behind Picbreeder resulted in a more unified representation of what the models generated. In that work, this disparity was attributed to the nature of SGD—which is used in training today’s large models—in contrast to how evolutionary processes result in a more factored internal world. At the very least, the fracturedness of these representations would make us question what kind of internal geometry human cognition might exhibit. Though speculative, perhaps the manifold model would be a viable candidate to capture this unification present within human cognition which enables us to function the way we do? Perhaps humans learn in a way that naturally builds continuous internal spaces? It should be noted that our understanding of human internal geometry is limited and so should be seen only as an analogy. I haven’t arrived at a solution. The question still remains unanswered and a resolution would perhaps help clarify what we even mean by AGI moving forward. What if general intelligence is—bluntly put—nothing more than the continuity of a mind’s internal landscape? References [1] Stanley, K. O., Lehman, J., Clune, J. (2025). https://arxiv.org/abs/2505.11581"
  },
  {
    "id": "index.html",
    "title": "Epicentre",
    "url": "./index.html",
    "tags": [],
    "date": "",
    "excerpt": "Cogito ergo sum. Je pense donc je suis. I think therefore I am. This would be the Cogito popularised by Descartes. Here he makes an attempt at answering the fundamental question, “Who am I?”. Who am I indeed. This incessant question gnaws at the corners of our mind, begging for clarity from a silent, indifferent universe…",
    "content": "Introduction Cogito ergo sum. Je pense donc je suis. I think therefore I am. This would be the Cogito popularised by Descartes. Here he makes an attempt at answering the fundamental question, “Who am I?”. Who am I indeed. This incessant question gnaws at the corners of our mind, begging for clarity from a silent, indifferent universe. Descartes’ Cogito serves as an attestation of humanity’s attempt at reasoning with the absurd. Was the uncovered “truth” satisfactory? Or do we have to question further? An answer to this is immediate, yet it eludes us, escapes us, and flees into oblivion. The universe is silent. Why are we here? What justifies our existence? These questions haunt us; they torment us. To paraphrase Descartes, we find ourselves in a whirlpool, tossed about, unable to swim to the top or touch the bottom with our feet. And yet, the search itself is not devoid of meaning. Out from the pithos ran rampant despair, negation, and alienation, yet something stayed with us. Something truly human. Hope. Hope retains an air of waywardness: a true embodiment of our rebellion. The Sisyphean struggle which elucidates this futile tug of war against the desolate silence of a meaningless universe. Perhaps the questions matter more than the answers. Perhaps the act of questioning is what makes us human. Perhaps an “I” would never be found in the depths of this infinite regression. I leave you something that may or may not persist beyond my time. A collection of ideas and thoughts encapsulating the human experience. My experience. The questions I thus ask cannot be unasked. Dear reader, you have been warned—stare into the basilisk only if you dare. This would be an effort at trying to grapple with reality itself, a means of finding solace in an absurdist creation, affirming the meaningless and loving one’s fate. Between absolute negation and the absurdist’s wager, I choose the latter. I hope my reasons will be apparent with every iterative essay and work I hence publish. Let us question everything and see where that leads. I give you only questions, not answers. The intention is to explore various facets of what makes humans, human. One must first exist to ask the questions. We have achieved the first task in the syllogism. Now, we shall ask the questions that follow."
  },
  {
    "id": "Into the Inferno.html",
    "title": "Into the Inferno",
    "url": "./Into the Inferno.html",
    "tags": [],
    "date": "",
    "excerpt": "Batter my heart, three-person'd God, for you As yet but knock, breathe, shine, and seek to mend; That I may rise and stand, o'erthrow me, and bend Your force to break, blow, burn, and make me new…",
    "content": "Batter my heart, three-person'd God, for you As yet but knock, breathe, shine, and seek to mend; That I may rise and stand, o'erthrow me, and bend Your force to break, blow, burn, and make me new. I, like an usurp'd town to another due, Labor to admit you, but oh, to no end; Reason, your viceroy in me, me should defend, But is captiv'd, and proves weak or untrue. Yet dearly I love you, and would be lov'd fain, But am betroth'd unto your enemy; Divorce me, untie or break that knot again, Take me to you, imprison me, for I, Except you enthrall me, never shall be free, Nor ever chaste, except you ravish me. - John Donne - Introduction The problem that persists today is not that meaning has abandoned us, but that the conditions under which it once appeared are no longer valid. It used to be the case that meaning was provided, the axiomatic priors defined, and all that followed were inferences from them. Yet over time, cracks slowly began to appear in this façade of transcendental meaning, until it could no longer hold. The death of God, brought about by man, and the tragedies of contemporary history fractured the inherited unity of meaning. What followed was not clarity, but proliferation. We find ourselves at a crossroads, unable to know for certain whether any path still leads beyond this condition—if such an exit exists at all. But what exactly led us to this condition? Was the search for transcendental meaning ever present? Or were we complacent with that which was given? Why did we ever refute it? If these questions are to be faced honestly, we must trace them downward to their origins... Metaphysical Submission There was a time when questioning the axiomatic priors of faith was not an intellectual exercise but a transgression. Meaning was not sought but received—cosmically guaranteed, externally anchored, and independent of individual assessment. Within such a framework, doubt did not function as permitted inquiry; its role was disruption. The origins of any such metaphysical order will not concern us here. What does matter is the condition that followed from it—a world in which meaning came before the individual and where one’s purpose was not to generate meaning, but to conform to it. The camel—evoked by Nietzsche—symbolises this state of metaphysical submission, the willingness to bear the weight laid upon it, the acceptance of duty, law and tradition. This is not to say that the camel is weak—quite far from it—but the camel does not question the value of the burden. It proves itself by carrying it. But the camel can only travel so far. The metamorphosis of the camel was demanded by lucidity. By attempting to reconcile lived reality and inherited abstraction, incompatibility emerges. The camel no longer believes. Icarian Liberation “Yesterday’s meditation raised doubts—ones that are too serious to be ignored—which I can see no way of resolving. I feel like someone who is suddenly dropped into a deep whirlpool that tumbles him around so that he can neither stand on the bottom nor swim to the top.” - René Descartes : Meditations on First Philosophy - The lucidity of Descartes, evident in his encounter with radical doubt, reveals just how disorienting this confrontation can be. Regardless of its apparent empowerment from the outset, the bittersweet truth is that it unmoors us and throws us into a state of vertigo. Allowing doubt to nestle within our psyche does not simply negate a specific set of beliefs; it rids us of the authority that once made belief stable. No inherited form of consolation remains intact. We have brought about this condition ourselves, and it cannot simply be undone. With solid ground now gone, we find ourselves suspended—no longer bound by inherited command, yet not yet capable of creation. All that we have achieved is lucidity and not yet the strength to face what it reveals—the possibility that reality may not be teleologically ordered with us in mind. This realization is liberation, but of a precarious kind. Not arrival but altitude. Not freedom secured but freedom unmoored. In this condition, we resemble Icarus. Returning once more to Nietzsche’s symbols, we now notice that the camel has transformed into the lion—the one who doubts, the one who rebels. The spirit of the lion acquires the power to say no—to inherited values, sacred commandments, and imposed meaning. The lion dismantles inherited systems; but erects nothing in its place. We rise—like Icarus—without knowing what lies beyond this destruction. Only now does the danger of flight reveal itself. Beyond Good and Evil “He who fights with monsters might take care lest he thereby become a monster. And if you gaze for long into an abyss, the abyss gazes also into you.” - Friedrich Nietzsche : Beyond Good and Evil - This dismantling of transcendental meaning does not, with it, bring any form of solace; nor does it amplify the chaos. We sit in silence with a purpose for our existence yet to be negotiated. Within the depths of foundational questioning we only find void. Even though there seems to be no justification—we continue to speak in the language of value, try to reason about it and even build on top of a foundation that no longer exists. Existence, to this end, appears structurally unsupported. We notice that the negation of inherited meaning has not liberated us in the way we hoped that it would. It has only produced suspension. This is nihilism—not the refusal of meaning, but the loss of ground to stand on. Habitual activities go on, perhaps as cultural residue, yet they are no longer rooted in metaphysical transcendence. They now await either abandonment or reconstitution Upon discovering this silence of the abyss, the human spirit does not respond uniformly. Ivan Karamazov, unable to reconcile worldly suffering with transcendental omni-benevolence, returns the ticket—not in triumph but in anguish. He rejects the narrative he inherited, yet finds no alternative ground upon which to stand. Some deliquesce under this tension. Svidrigailov sees the absence of transcendence as permission to make the world one’s playground—delving into indulgence. If there truly is no moral arbiter then anything is permitted. Yet even this hedonic reaction to an unbearable realization contains an air of exhaustion instead of contentment. The most extreme of reactions can be seen in Kirilov. If no higher power commands our negation then assuming its role and becoming the executioner would be the ultimate act of freedom. This reasoning, by asserting total freedom, results in total annihilation. But despite these responses varying in temperament, one thing remains constant. That is, the discovery that the abyss does not answer. Not before, not after. It continues to remain indifferent to our reactions. It does not rage. It remains silent. Our newfound realization does not give us peace of mind, yet this is the consequence we now inhabit. The descent is now complete."
  },
  {
    "id": "Multiple Linear Regression.html",
    "title": "Multiple Linear Regression",
    "url": "./Multiple Linear Regression.html",
    "tags": [],
    "date": "",
    "excerpt": "This is an extension of the work done previously in my report on univariate linear regression—a transition from univariate to multivariate formulations. Regression models form a pedagogical introduction to machine learning as they are among the most elementary models capable of attuning their parameters based on labeled data…",
    "content": "Introduction This is an extension of the work done previously in my report on univariate linear regression—a transition from univariate to multivariate formulations. Regression models form a pedagogical introduction to machine learning as they are among the most elementary models capable of attuning their parameters based on labeled data. Although transformer-based models take the limelight in contemporary AI discourse, linear regression serves as a stepping stone just as algebra serves as a precursor to calculus. Hence, this report explores an application of linear regression—multiple linear regression—on a salary prediction dataset to evaluate its predictive performance on unseen data. This project demonstrates a real-world application of multiple linear regression on the problem of salary estimation via the utilization of a publicly available dataset on employee salaries from Kaggle. This implementation, unlike in the univariate case, considers multiple features that may affect the target variable (salary). Data preprocessing was carried out by employing one-hot encoding in the case of categorical data and cyclic encoding using sine/cosine values for features with presumed cyclic structure. Z score normalization was explored as a means of scaling the features to improve stability in optimization and enhance convergence speed. Ridge regularization was implemented and tested in order to assess its value in the absence of polynomial features. The model’s ability to generalize was evaluated via an 80/20 train-test split, with metrics including MSE, RMSE, MAE and $R^2$. All components were implemented from the ground up relying only on core python and NumPy, avoiding the reliance on high-level ML libraries. Hence the overarching objective of this exercise was to gain a foundational understanding of the intricacies of multiple linear regression—unveiling the abstractions. Though pedagogically valuable, univariate linear regression’s capacity to handle most real world regression tasks remains quite limited. The earlier report—while assuming a single-variable dependency—explored the relationship between years of experience of an employee and their salaries with the intent on achieving generalizability. The inherently multivariate nature of salary prediction, with its diverse features ranging from categorical to numerical, demands a better solution. Multiple linear regression, by design, is capable of accommodating such complexity by taking diverse features into account through techniques like one-hot encoding and sine/cosine encoding for cyclic data. Hence, the decision to choose an MLR model for this problem can be justified. Problem Statement Salary prediction, given its multivariate dependence, requires a model which can capture complex relationships between these variables. The Kaggle dataset, used in this report, contains employee related features presumed to affect salary. The objective is to construct a model that captures the relationship between features and salaries. Furthermore, it should be noted that the dataset contains data from several data-types—categorical, numerical and cyclic data. Hence, this project seeks to build a model that is capable of learning from diverse types of data and generalizing on unseen data as well. Dataset Description The dataset used in this report to train the multiple linear regression model was sourced from Kaggle and was published by Ivan Gavrilov under the title “Factory’s Salary”. The dataset is composed of various features spanning different data-types with the addition of the target variable—salary. The features include date (of payment), profession, rank, equipment (used), insalubrity and size_production (interpreted here as the individual’s or their profession’s contribution to the factory’s output). The date feature exhibits cyclic properties; for instance, the month January, insofar as months are concerned, is adjacent to December even though numerically distant, and hence cyclic encoding was employed at the month level. Both the features profession and equipment are categorical features and were therefore one-hot encoded to make them suitable for the model. The rest of the data are all numerical and were directly utilized in training. With the features distinguished, data-types discerned and preprocessing techniques identified, the following section details the modeling pipeline that was used. Methodology Pre-Processing A CSV reader was implemented using basic python logic following the same from first principles ethos. The header row was disembodied from the dataset and used for feature identification later in the pipeline. Features and targets were separated for model training purposes as well. The dataset consisted of cyclic, categorical and numeric data and hence relevant preprocessing techniques were utilized. Cyclic encoding by means of representing the dates—more specifically the months—as points on a unit circle ensured that valuable information regarding when the salary was provided was not lost. As mentioned previously, if the elapsing of months was assumed to be linear then the fact that January follows December given the periodic nature of months, would be lost. Hence the cyclic encoding : For month $m \\in \\{1,\\dots,12\\}$: \\[ m_{\\sin}=\\sin\\!\\left(2\\pi \\frac{m}{12}\\right),\\qquad m_{\\cos}=\\cos\\!\\left(2\\pi \\frac{m}{12}\\right). \\] Furthermore, there were two columns of categorical data, namely the profession and the equipment columns. These were converted into numerical data by one-hot encoding them into dummy variables. One category per one-hot group was omitted to avoid perfect multicollinearity with the intercept term and ensure parameter identifiability. After converting the raw data into numerical representations, the examples were shuffled and split into an 80/20 train-test split. In order to z-score normalize, the sample mean and sample standard deviation were computed per continuous numerical feature—excluding one-hot encoded variables and cyclic sine/cosine features. (It should be noted that all data shuffling processes and other random operations were carried out using a fixed random seed to ensure reproducibility of results). The continuous features in the training split were standardized accordingly and the same training statistics were then applied to the test split in order to prevent data leakage. One-hot variables were left in their binary form as well as the sine/cosine features as they were already bounded in [-1,1] and were not standardized. This process produced the curated design matrix $X$ and target vector $y$ used in training and model evaluation. Model Selection Having converted the data contained within the dataset into their respective numerical representations, the decision on how to model the underlying structure has to be evaluated. In order to do this, a justifiable prima facie would be analyzing whether the marginal relationships between the continuous features and target exhibit approximate linearity. The above graph suggests a possible linear trend between rank and salary The above graph shows a possible linear relationship between insalubrity and salary The above graph exhibits an approximately linear association between size_production and salary Marginal linear trends, by nature, disregard interactions between features, conditional dependencies and how their dependent nature influences the target. Yet, the presence of such linear trends does not prima facie negate the assumed linear structure of the data and thus their existence suggests a linear hypothesis viable even though higher-order structure may later be required. Collectively, these marginal trends motivate the use of MLR as a first-order model capable of aggregating these approximate-linear effects within a unified framework. Despite marginal linear trends being examined only for continuous features, categorical and cyclic variables were incorporated through encodings that are conducive to linear models. One-hot encoding introduces binary indicator variables whose coefficients correspond to fixed additive offsets relative to a reference category (the coefficient gets added when an indicator is active and it simultaneously offsets the baseline). At the same time, the representation of a month as $(\\sin\\theta,\\cos\\theta)$ allows the model to express a smooth seasonal component of the form $a\\sin\\theta + b\\cos\\theta$ using a linear combination of these transformed features. Based on these observations—the presence of marginal linear trends and the compatibility of the employed encodings with linear models—we can, justifiably, assume that multiple linear regression is an appropriate first-order hypothesis for this salary prediction task. Model Definition This problem involves a feature vector with $p=18$ features and $m=264$ training examples. Accordingly we define the MLR model as follows: \\[ \\hat y_i = b + \\sum_{j=1}^{p} w_j\\,x_{ij}, \\qquad i = 1,2,\\dots,m \\] or equivalently: \\[ \\hat y_i = \\mathbf{x}_i^\\top \\mathbf{w} + b \\] In vector form, where $m$ : number of examples $p$ : number of features $x_{ij}$ : value of feature $j$ for example $i$ $w_{j}$ : weight associated with feature $j$ $b$ : bias (intercept) $\\hat y_i$ : predicted output for the $i$-th example Regularization and Cost Function Definition In its unregularized form, OLS (Ordinary Least Squares) does not impose any constraint on the magnitudes of the learned weights; just that the cost is minimized. There are situations where several features are correlated and not independent. In such cases, the distribution of weight mass becomes unstable under small changes in the training sample, particularly in how weight mass is distributed across correlated features. In such cases, each training run could result in markedly different weight parameters. These weights could fit the training set well but would fail to generalize. i.e. OLS optimizes training error alone and is indifferent among multiple solutions with similar fit. A resolution to this limitation of OLS is L2 or Ridge regularization. It augments the loss function with a penalty proportional to the squared Euclidean norm of the weight vector. Regularized Cost: \\[ J(\\mathbf{w}, b) = \\frac{1}{2m}\\sum_{i=1}^{m}\\left(\\mathbf{x}_i^\\top \\mathbf{w} + b - y_i\\right)^2 \\;+\\; \\frac{\\lambda}{2m}\\,\\|\\mathbf{w}\\|_2^2. \\] \\[ \\|\\mathbf{w}\\|_2^2 = \\sum_{j=1}^{p} w_j^2. \\] This augmented cost function presents itself as a tradeoff—rather than optimizing either criterion in isolation, (fitting the training data versus reducing the Euclidean norm of the weight vector), it incorporates both considerations into a single objective function, striking a balance between the two. That is, it seeks weights that fit the training data while penalizing solutions with large norms. This selection often—but not always—corresponds to distributing weight more smoothly across correlated features. Training Algorithm For $\\lambda > 0$, the L2 regularized cost function is strictly convex and therefore possesses a unique global minimum. Due to the small dataset size (\\(m=264\\)), batch gradient descent was used, as the full gradient could be computed efficiently, yielding a stable, deterministic optimization trajectory. In contrast, modern large-scale neural networks (including transformer LLMs) often employ stochastic or mini-batch gradient methods, where gradients are estimated from subsets of data to reduce computation per update; this distinction is briefly discussed in the Appendix. The update rule for each weight and bias term is defined as: \\[ \\begin{aligned} w_j &\\leftarrow w_j - \\alpha\\left(\\frac{1}{m}\\sum_{i=1}^{m}(\\hat y_i-y_i)x_{ij}+\\frac{\\lambda}{m}w_j\\right), \\qquad j=1,\\dots,p,\\\\ b &\\leftarrow b - \\alpha\\left(\\frac{1}{m}\\sum_{i=1}^{m}(\\hat y_i-y_i)\\right). \\end{aligned} \\] where, $\\hat y_i = \\mathbf x_i^\\top \\mathbf w + b$. Each update moves the parameters in the direction of the steepest local decrease, i.e. the direction given by the negative gradient evaluated at the current parameters. (Refer to Appendix) The weights and bias were initialized to zero before running the training algorithm. Upon experimenting with iteration count, learning rate and the regularization constant, the following choices were made for each hyper-parameter. Iterations = 1000000 $\\alpha$ = 5e-6 $\\lambda$ = 10 These values were selected empirically by monitoring the convergence of the training loss as well as final test-set evaluation metrics. Appendix Perfect Multicollinearity Assume that we have a multiple linear regression (MLR) model with p+1 features. Assume that p of these features are numerical and the other is categorical with K number of labels which are one-hot encoded as D_k for k=1,2,…,K. Then the MLR model can be given as follows: \\[ \\hat y = \\beta_0 + \\sum_{j=1}^{p}\\beta_j x_j + \\sum_{k=1}^{K}\\gamma_k D_k, \\] where $\\beta_0$ is the intercept of the model, $\\beta_j$ for j = 1,2,…,p are the weights of the numerical features and $\\gamma_k$ for k=1,2,…,K are the weights for the one hot encoded labels. (Note that we are considering all the labels within the category). Also note that for a given D_k, $D_k\\in\\{0,1\\}$ and \\[ \\sum_{k=1}^{K} D_k = 1. \\] Now, since $\\sum_{k=1}^{K} D_k = 1.$ we can write $\\beta_0$ as : \\[ \\beta_0 = \\beta_0 \\cdot 1 = \\beta_0 \\sum_{k=1}^{K} D_k. \\] and substituting this value back into the original equation gives : \\[ \\hat y = \\sum_{j=1}^{p}\\beta_j x_j + \\sum_{k=1}^{K}\\gamma_k D_k + \\beta_0\\sum_{k=1}^{K} D_k = \\sum_{j=1}^{p}\\beta_j x_j + \\sum_{k=1}^{K}(\\gamma_k+\\beta_0)D_k. \\] This shows that the intercept could be “absorbed” into the dummy coefficients because the dummies always sum to 1."
  },
  {
    "id": "Process of inquiry.html",
    "title": "Process of inquiry",
    "url": "./Process of inquiry.html",
    "tags": [],
    "date": "",
    "excerpt": "In the depths of pondering we found within us, not any remedy for this aching call for clarity, but more turbidity added into the mix. This problem does not arise in a vacuum but is the culmination of millions of years of evolution, genetic and cultural, leading up to this precise moment where we find ourselves demanding clarity…",
    "content": "First Entry In the depths of pondering we found within us, not any remedy for this aching call for clarity, but more turbidity added into the mix. This problem does not arise in a vacuum but is the culmination of millions of years of evolution, genetic and cultural, leading up to this precise moment where we find ourselves demanding clarity. But the long chain preceding us is not of today’s concern. Today, I intend to explore the thoughts that lead up to this question. How idyllic life becomes a prison. That moment of lucidity which demands reasoning. Alas there is nothing to reason. All is but randomness and nothing more. Yet understanding how we got here will be of some solace and so explore we shall. It begins with a dissatisfaction of the routinely structure we mould for ourselves. The irony lies in the purpose of our thus created prison. We craft this repetitive structure to find comfort, but the thing that is designed to console us births within us an off putting sensation. Whence does this come from? Why has this risen from the depths? Perhaps the question was always lurking in the shadows yet we were preoccupied with the calamity, that it merely faded into the background. Now that the routine is established we have created for ourselves a safer environment to glare into the abyss. Hence the more daunting questions can be asked. One must exist to question. Here we are and so we shall ask the questions."
  },
  {
    "id": "results.html",
    "title": "Search Results",
    "url": "./results.html",
    "tags": [],
    "date": "",
    "excerpt": "",
    "content": ""
  },
  {
    "id": "Shower Thoughts.html",
    "title": "Fragments",
    "url": "./Shower Thoughts.html",
    "tags": [],
    "date": "",
    "excerpt": "Prologue Below lies a collection of my ideas—nascent in origin—listed under their rightful sections. It would be difficult to refer to it as anything other than \" \" as it truly is just that.   of my soul, unfinished at times, all an attestation to the person I am. Expressions, contemplations, and analyses—each reflecting a facet of my being…",
    "content": "Prologue Below lies a collection of my ideas—nascent in origin—listed under their rightful sections. It would be difficult to refer to it as anything other than \"Fragments\" as it truly is just that. Fragments of my soul, unfinished at times, all an attestation to the person I am. Expressions, contemplations, and analyses—each reflecting a facet of my being. To be candid, the topics contained here and even the forms of expression are not of one brood, but born of varied genres and moods. Since I gravitate heavily towards the world views of Camus, one could call this a sort of proliferation of intellectual tastes. I hope you find this enticing and thought provoking. Simmering Conceptions Pondering Upon General Intelligence Nascent ponderings"
  },
  {
    "id": "The Great Red Dragon.html",
    "title": "The Great Red Dragon",
    "url": "./The Great Red Dragon.html",
    "tags": [],
    "date": "",
    "excerpt": "A work in progress. Please await completion...",
    "content": "A work in progress. Please await completion..."
  },
  {
    "id": "Univariate Linear Regression.html",
    "title": "Univariate Linear Regression",
    "url": "./Univariate Linear Regression.html",
    "tags": [],
    "date": "",
    "excerpt": "The black box LLMs—widely used yet poorly understood—underscore the necessity of understanding the core mechanisms that underpin their behavior and their astute capabilities of next token prediction…",
    "content": "Abstract The black box LLMs—widely used yet poorly understood—underscore the necessity of understanding the core mechanisms that underpin their behavior and their astute capabilities of next token prediction. The scrupulous analysis of univariate linear regression--which acts as a sort of rite of passage into the world of AI--implemented from first principles accompanied with rigorous documentation of the procedure and theory perhaps may help elucidate how these models, which scale up to incomprehensible sizes with billions of parameters, behave at a fundamental level. An example of salary versus experience was selected as the use case for this model born out of Python and enriched by a publicly available dataset on Kaggle. The training was conducted via the utilization of gradient descent accompanied by a mean squared error cost function, a learning rate of 1e-3, and 100,000 iterations. Convergence was achieved upon the selection of the aforementioned gradient descent parameters whilst giving insight into how such a model works. Introduction The advent of artificial intelligence in the 21st century has made it essential that practitioners and thinkers alike understand the foundations of machine learning. This resurgence in interpretability has led me to explore AI and machine learning from a different lens which most contemporaries in the field seldom wear. Initiating from the simplest of models and extending this form of scrupulous analysis to modern day black boxes may help align these models in such a way that is conducive to human flourishing. Among the simplest and most instructive models in this regard is linear regression— specifically, its univariate form. This project explores the relationship between an employee's years of experience and their estimated salary using a univariate linear regression model. A publicly available dataset was used, and the model was implemented in Python using NumPy. Although convenient, relying on high-level libraries was disregarded and the algorithm was constructed from first principles, including the cost function and gradient descent optimization process. These steps were followed in order to ensure a more robust understanding of the underlying mechanics that govern even the simplest of predictive models. Dataset Description The dataset used in this technical report was sourced from Kaggle, titled “Salary Prediction Data - Simple linear regression” by Krishna Raj. Designed specifically for introductory regression tasks, it serves as an ideal foundation for developing and testing a univariate linear regression model. The dataset comprises of 30 examples with a single feature and a single target being years of experience and salary earned respectively. Both the feature and the target variables being continuous variables. No preprocessing was required, as the dataset was already clean and well-structured, that is, it contained no missing values, both the features and targets were numeric and appropriately scaled, and the data was compact and tidy. Methodology Model Definition : Since the focus is on a univariate linear regression—where experience is the feature and salary is the target— the model is defined as, \\[ f\\left(x^{(i)}\\right) = w x^{(i)} + b, \\quad i = 1, 2, \\dots, m \\] where w and b are the model parameters which can be manipulated, $x^{(i)}$ is the value of the feature of the i'th training example and $f(x^{(i)})$ is the prediction made by the model of the target (salary) for the i'th training example. Here m is the total number of examples within the dataset. Cost Function Definition : As the desired output is to discern a model that is adept at explaining the dataset we first need to compute the model's error. To achieve this we use a function called the Cost Function which is of the following form. \\[ J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( wx^{(i)} + b - y^{(i)}) \\right)^2 \\] Here we compute the squared difference between the prediction $f(x^{(i)})$ and the target $y^{(i)}$ and divide it by 2m where m is the total number of training examples. The squared term penalizes larger errors and the division by m averages the cost making it independent of the number of training examples. This converts the total squared error into an average squared error, which makes comparing between datasets more convenient. The inclusion of a 2 within the denominator will see its importance once the gradient descent algorithm is implemented. Purpose of Cost Function: As we've now achieved a method to evaluate the deviation of our model with regard to the dataset, we now require a process which reduces this deviation. Such an effect is ensured by the process of gradient descent. Gradient Descent Introduction : The parameters of the model are its weight and bias and thus by adjusting these the nature of the model can be altered. Since we are already well equipped with a methodology to measure the deviation of the model we minimize the cost—the deviation—by adjusting the model parameters. The cost function is a function of two variables and hence we need to minimize it with respect to both these variables. From calculus, the partial derivative of a function with respect to one of its variables describes the rate of change of the function accordingly as well as the gradient of that function in the same regard. We implement gradient descent as follows. Update Rule for w : First let us observe how the weight needs to be adjusted to minimize the cost \\[ w := w - \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\] In each step of gradient descent, the parameter is updated by subtracting the learning rate times the gradient of the cost function. What this implementation ensures is that if the preexisting weight parameter of the model is one that results in a higher cost than the minimum value, it reduces that value for the weight which results in an overall reduction of the cost. If the cost function is plotted against w, it can be seen that when the gradient of the function is positive w needs to be reduced and when the gradient is negative, w needs to be increased. The negative sign in the gradient descent step ensures this. The learning rate $\\alpha$ plays a crucial role. It dictates by how much the value of w is altered. For lower values of $\\alpha$, the change is small but for larger values of $\\alpha$ the change is also large. The selection of an apt value for $\\alpha$ (the learning rate) requires a bit of experimentation and it should be noted that a larger value may lead to gradient descent overshooting and diverging from the local—in our case, global—minima whereas a smaller value for the learning rate would result in more time taken to reach the local minima. The above graphs illustrate the cost function’s behavior with respect to the weight and bias, as well as their combined effect in a 3D surface plot. The convexity of the cost function can be understood based on its parabolic profiles and its bowl-shaped surface with regard to w, b and their combined effect. This property ensures that gradient descent converges reliably to the global minimum despite the starting point. (Note that the above cost function is not descriptive of the data concerned in this report.) The squared difference between the model prediction and the target is the basis of the cost function and hence when considered with respect to w, the cost function exhibits a parabolic structure resulting in a convex function. When the cost function is expressed as a function of both w and b, it resembles a bowl-shaped surface in the cost space unlike non-convex surfaces which may contain multiple minima. Convexity ensures that any local minimum is a global minimum, making gradient descent both effective and reliable for optimization in this context. This property is especially beneficial in our case guaranteeing that gradient descent will converge to the global minimum, regardless of initial parameter values. The same is true for the model parameter b and the implementation of gradient descent is identical. \\[ \\frac {\\partial J(w,b)} {\\partial w} = \\frac {1}{m} \\sum_{i=1}^{m} \\left(f(x^{(i)}) - y^{(i)} )x{(i)}\\right) \\] \\[ \\frac {\\partial J(w,b)} {\\partial b} = \\frac {1}{m} \\sum_{i=1}^{m} \\left(f(x^{(i)}) - y^{(i)} )\\right) \\] The partial derivatives of the cost function with respect to each parameter are given above. The inclusion of a 2 within the cost function resulted in the above expressions for the partial derivatives of the cost function. It cancels out with the 2 present in the numerator of the derivative resulting in a cleaner function devoid of extra constants increasing clutter. Convergence and Training : Gradient descent is run until the cost function reaches convergence and so the model may provide an optimal fit to the dataset under the defined cost metric. Gradient descent, in my implementation, ran over 100,000 iterations and the learning rate was set to 1e-3. These parameters were selected after experimenting with differing learning rates and iteration counts and hence yielded results devoid of divergence or oscillation around the minimum, that is, this configuration achieved stable convergence. After the training is complete, the training data are used to assess the accuracy of the model’s prediction capabilities and the scatter plot for the dataset and the linear function of the model are plotted to verify this. Empirical Findings and Interpretation The above graph displays the original dataset alongside the model’s linear fit, illustrating the apparent linear relationship between experience and salary. Both graphical and terminal outputs were used in order to assess model performance. The plot depicts the model $y = wx +b$ overlaid on the dataset’s scatter points affirming a close fit. The proximity of the linear function to the observed values suggests strong explanatory capabilities. The terminal log tracked key values at every 1/10th of the training process: the cost, weight, and bias. Furthermore, the total time taken to train the model and the cost before and after the training run were displayed as well. Initial Cost : 3,251,477,635.366667 Final Cost : 15,635,475.861140488 Training Time : 0.8600s Final w : 9449.9623 Final b : 25792.2001 The substantial drop in cost—from the order of 1e+9 to 1e+6—reflects a successful convergence. Terminal output showing parameter updates and final training statistics. Discussion Given the apparent linear correlation between experience and salary, a univariate linear regression model was deemed both sufficient and illustrative. Upon making the selection the model was then adjusted accordingly to fit the dataset. A clean dataset, a descriptive cost function, and the implementation of gradient descent formed a harmonious triumvirate that enabled effective model training. Although there are more variables at play which lends to how much an individual is expected to make, opting to design a linear regression model for this exact use case seemed appropriate. The first principles approach used in this implementation helped understand the inner workings of a univariate linear regression and paves the way for future abstracting away of these functions. The intention being the demystification of the inner workings that underlines predictive intelligence—even in its simpler linear form. Both the cost function and gradient descent implementations were selected as mentioned earlier in the report due to their prowess at describing and training a univariate linear regression model. Although there were instances the code could have been made more concise by relying on an external library, the decision was made to opt for a first principle approach. This helps elucidate how intelligence—albeit a simpler version of it—could arise from basic logic and mathematics. It should be duly noted that although the relationship between salary and experience exhibits a linear relationship, it may not always be the case and other variables may contribute to the overall result. Hence this model should be scaled to a multiple linear regression model which captures as many variables as possible to give a solid description of real world data. Furthermore, the miniature nature of the dataset used might result in an overfitting of the model to the dataset and may not be reflective of the real world data. A larger dataset would help avoid this inconvenience. The insights were shaped through study, experimentation, and dialogue with an LLM assistant—ironically the very phenomenon this project seeks to comprehend. However, the final structure, logic, and analytical framing remain my own. Conclusion The intent of this seemingly pedantic but inherently insightful venture was to understand the inner workings of artificial intelligence through the lens of univariate linear regression. The construction and implementation of the model was carried out using Python in order to predict employee salary based on years of experience. A first-principles approach was adopted, constructing both the cost function and gradient descent algorithm without abstraction. Although a z-score normalizer was implemented, it was omitted since the single feature was already well scaled and applying it would have hindered interpretability. Hence the objective, as is apparent, was to explore the implementation of a univariate linear regression model and apply it in an appropriate example, that is salary v experience in our case. The implementation consisted of three phases: acquiring the dataset, defining a cost function and implementing gradient descent. The mean squared error was chosen as the cost function due to its effectiveness in quantifying deviation in linear models. The selected learning rate and iteration count led to stable convergence, as evidenced by the final cost and predictive fit. Graphical and terminal outputs were used to monitor key statistics such as, the cost and model parameter values every tenth of the total iterations, the cost before and after training and the time taken for convergence or rather to conclude gradient descent. The model’s final outputs confirmed that the chosen learning rate and iteration count ensured smooth convergence. The linearity of the dataset was described by the model and hence is adept at making predictions with considerably high accuracy. This elucidates the fact that intelligence—although not comparable in our case to its biological counterpart—could emerge through basic mathematics and logic. This highlights the necessity of demystifying the age old conundrum of the definition of intelligence. This model was applied in a field where more than one variable is at play. This implementation disregarded the importance of other factors such as the age of the employee, their qualifications, prior rejections, and other relevant factors. Furthermore, the dataset used comprised of only 30 training examples and may have caused the model to overfit the sample, limiting generalizability. A first principles approach, insightful though it may be, works antithetical to efficiency and hence the use of some high level libraries would have accelerated the workflow. Following the Ariadne’s string within the labyrinth of intelligence we discover that from raw data to predictive clarity, the emergent properties of even the simplest forms of intelligence are not sorcery, but method—shaped by reason, refinement, and a refusal to blindly abstract. References [1] Salary Prediction Data – Simple Linear Regression, Krishna Raj, Kaggle dataset. https://www.kaggle.com/datasets/krishnaraj30/salary-prediction-data-simple-linear-regression [2] Ng, A. (n.d.). Machine Learning Specialization. Coursera. https://www.coursera.org/specializations/machine-learning-introduction [3] Python Software Foundation. Python Language Reference. https://www.python.org [4] NumPy Developers. (n.d.). NumPy v1.26 Manual. https://numpy.org/doc/ [5] Chandrawansa, M. (2025). Univariate Linear Regression from First Principles [Source code]. GitHub. https://github.com/miniduofficial/univariate-linear-regression"
  }
]