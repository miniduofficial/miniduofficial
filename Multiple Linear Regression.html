<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="styles.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css">
  <link href="https://fonts.googleapis.com/css2?family=Abril+Fatface&family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&family=Volkhov:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
  <title>Multiple Linear Regression</title>
  <style>
    .captions {
        font-size: 1em;
        text-align: center;
        font-style: italic;
        margin-top: 0.5em;
        color: #A9A295; /* or any subtle tone that matches your aesthetic */
    }
    .compact-output{
        font-size: 1.1em;
    }

    @media (max-width:880px) {
      .page-title{
        font-size: 0.7em!important;

      }

      .search-bar{
        font-size: 0.8em;
      }
      .search-bar input{
        width: 16vh;
      }
    }
    @media (max-width:700px) {
      .page-title{
        font-size: 0.6em!important;
      }
    }

    @media (max-width:555px) {
        .page-title{
            font-size: 0.72em;
            margin-left: -3%;
        }
        .search-bar{
        font-size: 0.6em;
      }
      .search-bar input{
        width: 11vh;
      }
    }

    @media (max-width:428px) {
      .page-title{
        font-size: 0.3em!important;
        padding-left: 0px;
        margin-left: -12%;
      }
      .box2{
        height: 0.4vh;
        min-height: 5px;
        line-height: 0.2;
      }

      .blog-content h1{
        font-size: 1.4em;
      }

      .blog-content p{
        font-size: 0.9em;
      }

      .section-title h1{
        font-size: 1.5em;
      }

    .compact-output{
        font-size: 0.85em;
    }

    .blog-content .captions{
        font-size: 0.8em;
    }
    }
  </style>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
</head>
<body>
  <div class="page-wrapper">
    <main>
      <div id="loading-screen"  class="progressive-bg"  data-preview="Landscape-low.jpg"  data-src="Landscape.jpeg">
        <div class="loading-symbol">
          <img src="Skull.gif" alt="Loading Icon">
        </div>
        <p id="quote-display" class="loading-text">Loading...</p>
      </div>
      <div class="main-grid">
        <!-- Logo Section -->
        <div class="main-box" style="width: 200px; height: 200px; padding: 0%;">
          <a href="index.html">
            <div class="logo">
            <img src="logo1.png" alt="Logo" style="width: 100%; height: 100%; object-fit: cover;">
            </div>
          </a>
        </div>
    
        <!-- Blog Title and Navigation -->
        <div class="main-box" style="padding-top: 10px; font-size: 200%;">
          <header class="main-box" style="padding-top: 0px; font-size: 100%;">
            <div class="title">Noble Homer's Blog</div>
          </header>
    
          <!-- Navigation Menu -->
          <div class="menu-items">
            <a href="index.html" class="box" style="text-decoration: none; color: inherit;" >Epicentre</a>
            <a href="Essays.html" class="box" style="text-decoration: none; color: inherit;">Articulations</a>
            <a href="Shower Thoughts.html" class="box" style="text-decoration: none; color: inherit;">Fragments</a>
            <a href="Contact.html" class="box" style="text-decoration: none; color: inherit;">The Study</a>
            <div class="search-bar">
              <input type="text" placeholder="Ask the Oracle ..." aria-label="Search" id="search-input-large">
              <button type="button" id="search-button-large">
                <img src="Search.png" alt="Search">
              </button>
            </div>            
          </div>
        </div>
      </div>
      <div class="main-box2">
        <a href="index.html">
          <div class="logo2">
          <img src="logo1.png" alt="Logo">
          </div>
        </a>
      </div>
      <!-- Hamburger Menu -->
      <div class="menu-title-wrapper">
        <div class="menu-toggle">
          <span class="line"></span>
          <span class="line"></span>
          <span class="line"></span>
        </div>
        <div class="page-title" id="page-title">Multiple Linear Regression</div>
        <div class="search-bar">
          <input type="text" placeholder="Ask the Oracle ..." aria-label="Search" id="search-input-small">
          <button type="button" id="search-button-small">
            <img src="Search.png" alt="Search">
          </button>
        </div>
        <div class="menu-items-vertical" id="menu-items-vertical">
          <a href="index.html" class="box2" style="text-decoration: none; color: inherit; font-size: 0.7em;">Epicentre</a>
          <a href="Essays.html" class="box2" style="text-decoration: none; color: inherit; font-size: 0.7em;">Articulations</a>
          <a href="Shower Thoughts.html" class="box2" style="text-decoration: none; color: inherit; font-size: 0.7em;">Fragments</a>
          <a href="Contact.html" class="box2" style="text-decoration: none; color: inherit; font-size: 0.7em;">The Study</a>
        </div>
      </div>
    <script src="script.js"></script>
    <div class="section-title">
        <h1>Salary Estimation Using Multiple Linear Regression : From Abstraction to Implementation</h1>
    </div>
    <div class="blog-content">
        <h1>Introduction</h1>
        <p>	This is an extension of the work done previously in my report on univariate linear regression—a transition from univariate to multivariate formulations. Regression models form a pedagogical introduction to machine learning as they are among the most elementary models capable of attuning their parameters based on labeled data. Although transformer-based models take the limelight in contemporary AI discourse, linear regression serves as a stepping stone just as algebra serves as a precursor to calculus. Hence, this report explores an application of linear regression—multiple linear regression—on a salary prediction dataset to evaluate its predictive performance on unseen data.</p>
        <p>	This project demonstrates a real-world application of multiple linear regression on the problem of salary estimation via the utilization of a publicly available dataset on employee salaries from Kaggle. This implementation, unlike in the univariate case, considers multiple features that may affect the target variable (salary). Data preprocessing was carried out by employing one-hot encoding in the case of categorical data and cyclic encoding using sine/cosine values for features with presumed cyclic structure. Z score normalization was explored as a means of scaling the features to improve stability in optimization and enhance convergence speed. Ridge regularization was implemented and tested in order to assess its value in the absence of polynomial features. The model’s ability to generalize was evaluated via an 80/20 train-test split, with metrics including MSE, RMSE, MAE and $R^2$. All components were implemented from the ground up relying only on core python and NumPy, avoiding the reliance on high-level ML libraries. Hence the overarching objective of this exercise was to gain a foundational understanding of the intricacies of multiple linear regression—unveiling the abstractions.</p>
        <p>	Though pedagogically valuable, univariate linear regression’s capacity to handle most real world regression tasks remains quite limited. The earlier report—while assuming a single-variable dependency—explored the relationship between years of experience of an employee and their salaries with the intent on achieving generalizability. The inherently multivariate nature of salary prediction, with its diverse features ranging from categorical to numerical, demands a better solution. Multiple linear regression, by design, is capable of accommodating such complexity by taking diverse features into account through techniques like one-hot encoding and sine/cosine encoding for cyclic data. Hence, the decision to choose an MLR model for this problem can be justified.</p>
        <br>
        <h1>Problem Statement</h1>
        <p>	Salary prediction, given its multivariate dependence, requires a model which can capture complex relationships between these variables. The Kaggle dataset, used in this report, contains employee related features presumed to affect salary. The objective is to construct a model that captures the relationship between features and salaries. Furthermore, it should be noted that the dataset contains data from several data-types—categorical, numerical and cyclic data. Hence, this project seeks to build a model that is capable of learning from diverse types of data and generalizing on unseen data as well.</p>
        <br>
        <h1>Dataset Description</h1>
        <p>	The dataset used in this report to train the multiple linear regression model was sourced from Kaggle and was published by Ivan Gavrilov under the title “Factory’s Salary”. The dataset is composed of various features spanning different data-types with the addition of the target variable—salary. The features include date (of payment), profession, rank, equipment (used), insalubrity and size_production (interpreted here as the individual’s or their profession’s contribution to the factory’s output). </p>
        <p>	The date feature exhibits cyclic properties; for instance, the month January, insofar as months are concerned, is adjacent to December even though numerically distant, and hence cyclic encoding was employed at the month level. Both the features profession and equipment are categorical features and were therefore one-hot encoded to make them suitable for the model. The rest of the data are all numerical and were directly utilized in training. With the features distinguished, data-types discerned and preprocessing techniques identified, the following section details the modeling pipeline that was used. </p>
        <br>
        <h1>Methodology</h1>
        <h2>Pre-Processing</h2>
        <p>	A CSV reader was implemented using basic python logic following the same from first principles ethos. The header row was disembodied from the dataset and used for feature identification later in the pipeline. Features and targets were separated for model training purposes as well.</p>
        <p>	The dataset consisted of cyclic, categorical and numeric data and hence relevant preprocessing techniques were utilized. Cyclic encoding by means of representing the dates—more specifically the months—as points on a unit circle ensured that valuable information regarding when the salary was provided was not lost. As mentioned previously, if the elapsing of months was assumed to be linear then the fact that January follows December given the periodic nature of months, would be lost. Hence the cyclic encoding : </p>
        <p>For month $m \in \{1,\dots,12\}$:
\[
m_{\sin}=\sin\!\left(2\pi \frac{m}{12}\right),\qquad
m_{\cos}=\cos\!\left(2\pi \frac{m}{12}\right).
\]
        </p>
        <p>Furthermore, there were two columns of categorical data, namely the profession and the equipment columns. These were converted into numerical data by one-hot encoding them into dummy variables. One category per one-hot group was omitted to avoid perfect multicollinearity with the intercept term and ensure parameter identifiability.</p>
        <p>After converting the raw data into numerical representations, the examples were shuffled and split into an 80/20 train-test split. In order to z-score normalize, the sample mean and sample standard deviation were computed per continuous numerical feature—excluding one-hot encoded variables and cyclic sine/cosine features. (It should be noted that all data shuffling processes and other random operations were carried out using a fixed random seed to ensure reproducibility of results).</p>
        <p>The continuous features in the training split were standardized accordingly and the same training statistics were then applied to the test split in order to prevent data leakage. </p>
        <p>One-hot variables were left in their binary form as well as the sine/cosine features as they were already bounded in [-1,1] and were not standardized.</p>
        <p>This process produced the curated design matrix $X$ and target vector $y$ used in training and model evaluation.</p>
        <br>
        <h2>Model Selection</h2>
        <p>Having converted the data contained within the dataset into their respective numerical representations, the decision on how to model the underlying structure has to be evaluated. In order to do this, a justifiable prima facie would be analyzing whether the marginal relationships between the continuous features and target exhibit approximate linearity. </p>
        <img src="Rank v Salary.png" style="display: block; margin: 0 auto; width: auto; max-width: 95%;">
        <p class="captions">
            <i>The above graph suggests a possible linear trend between rank and salary</i>
        </p>
        <img src="Insalubrity v Salary.png" style="display: block; margin: 0 auto; width: auto; max-width: 95%;">
        <p class="captions">
            <i>The above graph shows a possible linear relationship between insalubrity and salary</i>
        </p>
        <img src="Size Production v Salary.png" style="display: block; margin: 0 auto; width: auto; max-width: 95%;">
        <p class="captions">
            <i>The above graph exhibits an approximately linear association between size_production and salary</i>
        </p>
        <br>
        <p>Marginal linear trends, by nature, disregard interactions between features, conditional dependencies and how their dependent nature influences the target. Yet, the presence of such linear trends does not prima facie negate the assumed linear structure of the data and thus their existence suggests a linear hypothesis viable even though higher-order structure may later be required.</p>
        <p>Collectively, these marginal trends motivate the use of MLR as a first-order model capable of aggregating these approximate-linear effects within a unified framework.</p>
        <p>Despite marginal linear trends being examined only for continuous features, categorical and cyclic variables were incorporated through encodings that are conducive to linear models. One-hot encoding introduces binary indicator variables whose coefficients correspond to fixed additive offsets relative to a reference category (the coefficient gets added when an indicator is active and it simultaneously offsets the baseline). At the same time, the representation of a month as $(\sin\theta,\cos\theta)$ allows the model to express a smooth seasonal component of the form $a\sin\theta + b\cos\theta$ using a linear combination of these transformed features. </p>
        <p>Based on these observations—the presence of marginal linear trends and the compatibility of the employed encodings with linear models—we can, justifiably, assume that multiple linear regression is an appropriate first-order hypothesis for this salary prediction task.</p>
        <br>
        <h2>Model Definition</h2>
        <p>This problem involves a feature vector with $p=18$ features and $m=264$ training examples. Accordingly we define the MLR model as follows:</p>
        <p>	\[
		\hat y_i = b + \sum_{j=1}^{p} w_j\,x_{ij}, \qquad i = 1,2,\dots,m
	\]</p>
        <p>or equivalently: </p>
        <p>	\[
		\hat y_i = \mathbf{x}_i^\top \mathbf{w} + b
	\]</p>
        <p>In vector form, where</p>
        <li><p style="padding-left: 100px;">$m$ : number of examples</p></li>
        <li><p style="padding-left: 100px;">$p$ : number of features</p></li>
        <li><p style="padding-left: 100px;">$x_{ij}$ : value of feature $j$ for example $i$</p></li>
        <li><p style="padding-left: 100px;">$w_{j}$ : weight associated with feature $j$</p></li>
        <li><p style="padding-left: 100px;">$b$ : bias (intercept)</p></li>
        <li><p style="padding-left: 100px;">$\hat y_i$ : predicted output for the $i$-th example</p></li>
        <br>
        <h2>Regularization and Cost Function Definition</h2>
        <p>In its unregularized form, OLS (Ordinary Least Squares) does not impose any constraint on the magnitudes of the learned weights; just that the cost is minimized. There are situations where several features are correlated and not independent. In such cases, the distribution of weight mass becomes unstable under small changes in the training sample, particularly in how weight mass is distributed across correlated features. In such cases, each training run could result in markedly different weight parameters. These weights could fit the training set well but would fail to generalize. </p>
        <p>i.e. OLS optimizes training error alone and is indifferent among multiple solutions with similar fit.</p>
        <p>A resolution to this limitation of OLS is L2 or Ridge regularization. It augments the loss function with a penalty proportional to the squared Euclidean norm of the weight vector.</p>
        <p>Regularized Cost:</p>
        <p>\[
J(\mathbf{w}, b)
=
\frac{1}{2m}\sum_{i=1}^{m}\left(\mathbf{x}_i^\top \mathbf{w} + b - y_i\right)^2
\;+\;
\frac{\lambda}{2m}\,\|\mathbf{w}\|_2^2.
\]

\[
\|\mathbf{w}\|_2^2 = \sum_{j=1}^{p} w_j^2.
\]      </p>
        <p>This augmented cost function presents itself as a tradeoff—rather than optimizing either criterion in isolation, (fitting the training data versus reducing the Euclidean norm of the weight vector), it incorporates both considerations into a single objective function, striking a balance between the two. That is, it seeks weights that fit the training data while penalizing solutions with large norms. This selection often—but not always—corresponds to distributing weight more smoothly across correlated features. </p>
        <br>
        <h2>Training Algorithm</h2>
        <p>For $\lambda > 0$, the L2 regularized cost function is strictly convex and therefore possesses a unique global minimum. Due to the small dataset size (\(m=264\)), batch gradient descent was used, as the full gradient could be computed efficiently, yielding a stable, deterministic optimization trajectory. In contrast, modern large-scale neural networks (including transformer LLMs) often employ stochastic or mini-batch gradient methods, where gradients are estimated from subsets of data to reduce computation per update; this distinction is briefly discussed in the Appendix.</p>
        <p>The update rule for each weight and bias term is defined as:</p>
        <p>	\[
		\begin{aligned}
		w_j &\leftarrow w_j - \alpha\left(\frac{1}{m}\sum_{i=1}^{m}(\hat y_i-y_i)x_{ij}+\frac{\lambda}{m}w_j\right), \qquad j=1,\dots,p,\\
		b &\leftarrow b - \alpha\left(\frac{1}{m}\sum_{i=1}^{m}(\hat y_i-y_i)\right).
		\end{aligned}
	\]  </p>
        <p>where, $\hat y_i = \mathbf x_i^\top \mathbf w + b$.</p>
        <p>Each update moves the parameters in the direction of the steepest local decrease, i.e. the direction given by the negative gradient evaluated at the current parameters. (Refer to Appendix)</p>
        <p>The weights and bias were initialized to zero before running the training algorithm. Upon experimenting with iteration count, learning rate and the regularization constant, the following choices were made for each hyper-parameter.</p>
        <li><p style="padding-left: 100px;">Iterations = 1000000</p></li>
        <li><p style="padding-left: 100px;">$\alpha$ = 5e-6</p></li>
        <li><p style="padding-left: 100px;">$\lambda$ = 10</p></li>
        <br>
        <p>These values were selected empirically by monitoring the convergence of the training loss as well as final test-set evaluation metrics. </p>
        <br>
        <h1>Appendix</h1>
        <h2>Perfect Multicollinearity</h2>
        <p>Assume that we have a multiple linear regression (MLR) model with p+1 features. Assume that p of these features are numerical and the other is categorical with K number of labels which are one-hot encoded as D_k for k=1,2,…,K.</p>
        <p>Then the MLR model can be given as follows:</p>
        <p>	\[
		\hat y = \beta_0 + \sum_{j=1}^{p}\beta_j x_j + \sum_{k=1}^{K}\gamma_k D_k,
	\]</p>
        <p>where $\beta_0$ is the intercept of the model, $\beta_j$ for j = 1,2,…,p are the weights of the numerical features and $\gamma_k$ for k=1,2,…,K are the weights for the one hot encoded labels. (Note that we are considering all the labels within the category).</p>
        <p>Also note that for a given D_k,  $D_k\in\{0,1\}$ and </p>
        <p>	\[
		\sum_{k=1}^{K} D_k = 1.
	\]
	
	<p>Now, since $\sum_{k=1}^{K} D_k = 1.$ we can write $\beta_0$ as :</p>
        <p>
	\[
		\beta_0 = \beta_0 \cdot 1 = \beta_0 \sum_{k=1}^{K} D_k.
	\]</p>
        <p>and substituting this value back into the original equation gives :</p>
        <p style="overflow: hidden;">	\[
		\hat y = \sum_{j=1}^{p}\beta_j x_j + \sum_{k=1}^{K}\gamma_k D_k + \beta_0\sum_{k=1}^{K} D_k = \sum_{j=1}^{p}\beta_j x_j + \sum_{k=1}^{K}(\gamma_k+\beta_0)D_k.
	\]</p>
        <p>This shows that the intercept could be “absorbed” into the dummy coefficients because the dummies always sum to 1.</p>
    </div>
    <button id="back-to-top" style="display: none;">
        <img src="Up.png" alt="Back to Top">
    </button>
  </main>
    <footer class="site-footer">
  <div class="footer-container">
    <nav class="footer-nav">
      <a href="index.html">Epicentre</a>&nbsp;&middot;&nbsp;
      <a href="Essays.html">Articulations</a>&nbsp;&middot;&nbsp;
      <a href="Shower Thoughts.html">Fragments</a>&nbsp;&middot;&nbsp;
      <a href="Contact.html">The Study</a>
    </nav>
    <p class="footer-credit">© 2025 Minidu Chandrawansa</p>
  </div>
</footer>
  </div>
  </body>
</html>